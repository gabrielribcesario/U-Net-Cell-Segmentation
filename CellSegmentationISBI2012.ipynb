{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net segmentation test\n",
    "## Dataset: https://www.kaggle.com/datasets/hamzamohiuddin/isbi-2012-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = True\n",
    "batch_size = 8\n",
    "max_epochs = 200\n",
    "checkpoint_dir = './models/ISBI2012/ckpt/'\n",
    "save_path = './models/ISBI2012/model.keras'\n",
    "load_path = './models/ISBI2012/model.keras'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc, os, cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet.utils import UNetHelper\n",
    "from unet.losses import IoU, dice_loss, unet_sample_weights\n",
    "from unet.augmentation import elastic_deformation, grid_deformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "\n",
    "lcm = lambda x, y: x * y // math.gcd(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set custom options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pandas.set_option('display.max_columns', None)\n",
    "#pd.pandas.set_option('display.max_rows', None)\n",
    "#np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy and Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed = lambda seed=42: tf.keras.utils.set_random_seed(seed)\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "if gpus <= 1: \n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=\"/GPU:0\")\n",
    "else: \n",
    "    strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())\n",
    "    \n",
    "n_devices = strategy.num_replicas_in_sync\n",
    "print(f'Using {n_devices} devices.')\n",
    "print(f'Using {strategy.__class__.__name__}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'cell_images/{0}'\n",
    "\n",
    "img_shape, img_mode = (512, 512, 1), cv2.IMREAD_GRAYSCALE #cv2.IMREAD_COLOR\n",
    "mask_shape, mask_mode = (512, 512), cv2.IMREAD_GRAYSCALE\n",
    "\n",
    "# 30 training samples, 30 test samples.\n",
    "data_len = 60\n",
    "data_type = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((data_len, img_shape[0], img_shape[1], img_shape[2]), data_type)\n",
    "y = np.zeros((data_len, mask_shape[0], mask_shape[1]), np.int32)\n",
    "sample_weights = np.zeros((data_len, mask_shape[0], mask_shape[1]), data_type)\n",
    "\n",
    "for i, (image, mask) in enumerate(zip(os.listdir(base_dir.format(f\"/train/imgs/\")), \n",
    "                                      os.listdir(base_dir.format(f\"/train/labels/\")))):\n",
    "    X[i] += np.expand_dims(cv2.imread(base_dir.format(f\"/train/imgs/{image}\"), img_mode), -1) / 255.\n",
    "    msk = cv2.imread(base_dir.format(f\"/train/labels/{mask}\"), mask_mode)\n",
    "    #_, msk = cv2.threshold(msk, 0, 255, cv2.THRESH_OTSU)\n",
    "    y[i] += msk.astype(np.int32) // 255\n",
    "\n",
    "    X[i + data_len // 2] = np.expand_dims(cv2.imread(base_dir.format(f\"/test/imgs/{image}\"), img_mode), -1) / 255.\n",
    "    msk = cv2.imread(base_dir.format(f\"/test/labels/{mask}\"), mask_mode)\n",
    "    #_, msk = cv2.threshold(msk, 0, 255, cv2.THRESH_OTSU)\n",
    "    y[i + data_len // 2] += msk.astype(np.int32) // 255\n",
    "\n",
    "    sample_weights[i] += unet_sample_weights(y[i], data_type=data_type)\n",
    "    sample_weights[i + data_len // 2] += unet_sample_weights(y[i + data_len // 2], data_type=data_type)\n",
    "\n",
    "gc.collect()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def pipeline(X, y, w):\n",
    "    y = tf.expand_dims(y, axis=-1)\n",
    "    w = tf.expand_dims(w, axis=-1)\n",
    "    # Horizontal flip.\n",
    "    if tf.random.uniform((), 0., 1.) >= 0.5:\n",
    "        X = tf.image.flip_left_right(X)\n",
    "        y = tf.image.flip_left_right(y)\n",
    "        w = tf.image.flip_left_right(w)\n",
    "    # Vertical flip.\n",
    "    if tf.random.uniform((), 0., 1.) >= 0.5:\n",
    "        X = tf.image.flip_up_down(X)\n",
    "        y = tf.image.flip_up_down(y)\n",
    "        w = tf.image.flip_up_down(w)\n",
    "    # Grid deformation.\n",
    "    if tf.random.uniform((), 0., 1.) >= 0.5:\n",
    "        grid_size = 5\n",
    "        distort_limits = (-.5, .5)\n",
    "        X = grid_deformation(X, distort_limits=distort_limits, grid_size=grid_size, order=1)\n",
    "        y = grid_deformation(y, distort_limits=distort_limits, grid_size=grid_size, order=0)\n",
    "        w = grid_deformation(w, distort_limits=distort_limits, grid_size=grid_size, order=0)\n",
    "    # Elastic deformation\n",
    "    if tf.random.uniform((), 0., 1.) >= 0.5:\n",
    "        alpha = 50.\n",
    "        sigma = 3.\n",
    "        auto_kSize = True\n",
    "        X = elastic_deformation(X, alpha=alpha, sigma=sigma, auto_kSize=auto_kSize, order=1)\n",
    "        y = elastic_deformation(y, alpha=alpha, sigma=sigma, auto_kSize=auto_kSize, order=0)\n",
    "        w = elastic_deformation(w, alpha=alpha, sigma=sigma, auto_kSize=auto_kSize, order=0)\n",
    "    return [X, tf.squeeze(y), tf.squeeze(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 15\n",
    "\n",
    "reset_seed(420)\n",
    "tmp = pipeline(X[i], y[i], sample_weights[i])\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(14,7))\n",
    "\n",
    "ax[0].set_title(\"Original\")\n",
    "ax[0].imshow(X[i], cmap=\"gray\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].set_title(\"Augmented\")\n",
    "ax[1].imshow(tmp[0], cmap=\"gray\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X[:data_len // 2], \n",
    "                                                y[:data_len // 2], \n",
    "                                                sample_weights[:data_len // 2]))\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X[data_len // 2:], \n",
    "                                             y[data_len // 2:]))\n",
    "\n",
    "train_ds = train_ds.map(pipeline, num_parallel_calls=tf.data.AUTOTUNE\n",
    "                        ).shuffle(train_ds.cardinality(), reshuffle_each_iteration=True\n",
    "                                  ).repeat(lcm(batch_size, data_len // 2) // (data_len // 2)).batch(batch_size, drop_remainder=False, num_parallel_calls=tf.data.AUTOTUNE\n",
    "                                                                                                    ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.batch(2 * batch_size, drop_remainder=False)\n",
    "\n",
    "train_ds = strategy.experimental_distribute_dataset(train_ds)\n",
    "val_ds = strategy.experimental_distribute_dataset(val_ds)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(helper, train_dataset, val_dataset=None, epochs=100, ckpt_every=10, plot_every=1):\n",
    "    history = []\n",
    "    ds_card = train_dataset.cardinality\n",
    "    for epoch in range(epochs):\n",
    "        epoch += 1\n",
    "        print(f'\\nEpoch {epoch}/{epochs}')\n",
    "            \n",
    "        if helper.opt_schedule is not None: \n",
    "            helper.optimizer.learning_rate = helper.opt_schedule(epoch)\n",
    "        progbar = tf.keras.utils.Progbar(target=ds_card)\n",
    "        for i, batch in enumerate(train_dataset):\n",
    "            i += 1\n",
    "            loss, acc = helper.dist_train_step(batch)\n",
    "            progbar.update(i, zip(['loss', 'acc'], [loss, acc]), finalize=False)\n",
    "\n",
    "        if val_dataset is not None:\n",
    "            val_loss, val_acc = 0.0, 0.0\n",
    "            for j, batch in enumerate(val_dataset):\n",
    "                vloss, vacc = helper.dist_val_step(batch)\n",
    "                val_loss += vloss; val_acc += vacc\n",
    "            val_loss /= (j+1); val_acc /= (j+1)\n",
    "            history.append([loss, acc, val_loss, val_acc])\n",
    "            progbar.update(i, zip(['loss', 'acc', 'val_loss', 'val_acc', 'lr'], \n",
    "                                [loss, acc, val_loss, val_acc, helper.optimizer.learning_rate.numpy()]), finalize=True)\n",
    "        else: \n",
    "            history.append([loss, acc])\n",
    "            progbar.update(i, zip(['loss', 'acc', 'lr'], [loss, acc, helper.optimizer.learning_rate.numpy()]), finalize=True)\n",
    "\n",
    "        if type(ckpt_every) is int:\n",
    "            if epoch % ckpt_every == 0: helper.checkpoint.save(helper.checkpoint_dir)\n",
    "            \n",
    "        if type(plot_every) is int:\n",
    "            if epoch % plot_every == 0:\n",
    "                plt.close()\n",
    "                idx = np.random.choice(np.arange(data_len // 2, data_len, 1))\n",
    "                image_list = [X[idx], y[idx], np.squeeze(helper.model(X[idx:idx+1], training=False).numpy().argmax(axis=-1))]\n",
    "                image_list = [(255. * img).astype('uint8') if img.dtype!='uint8' else img for img in image_list]\n",
    "                fig, ax = plt.subplots(1,3,figsize=(14,28))\n",
    "                ax[0].set_title(\"Image\")\n",
    "                ax[1].set_title(\"Mask\")\n",
    "                ax[2].set_title(\"Predicted Mask\")\n",
    "                for k in range(3): \n",
    "                    ax[k].imshow(image_list[k], cmap=\"gray\")\n",
    "                    ax[k].axis('off')    \n",
    "                plt.show()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#max_lr = 1.E-2\n",
    "max_lr = 1.E-3\n",
    "lr_decay_start, lr_decay_rate, lr_decay_step = (2, 0.1, 3)\n",
    "model_param = {\"input_shape\": img_shape,\n",
    "               \"dropout\": 0.2,\n",
    "               }\n",
    "\n",
    "try:\n",
    "    del helper\n",
    "except:\n",
    "    pass\n",
    "\n",
    "reset_seed()\n",
    "K.clear_session()\n",
    "\n",
    "with strategy.scope():    \n",
    "    gc.collect()\n",
    "    \n",
    "    helper = UNetHelper(strategy=strategy,\n",
    "                        model_param=model_param,\n",
    "                        loss_func=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                        optimizer=tf.keras.optimizers.SGD(learning_rate=max_lr, momentum=0.99),\n",
    "                        #opt_schedule=tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=[5,], values=[1e-2, 1e-3]),\n",
    "                        )\n",
    "    \n",
    "    if train_model:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            train(helper, train_ds, val_ds, max_epochs, ckpt_every=15, plot_every=30)\n",
    "        helper.model.save(save_path)\n",
    "    else: helper.model.load(load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    pred = helper.model.predict(X[data_len // 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.round wasn't working properly so I had to add a fuzz factor before rounding the results lol.\n",
    "print(f\"Average IoU on holdout set: {np.round(IoU(y[data_len // 2:], pred).numpy().mean() + 1E-10, 4)}\")\n",
    "print(f\"Average Dice loss on holdout set: {np.round(dice_loss(y[data_len // 2:], pred).numpy().mean() + 1E-10, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 36))\n",
    "subtitles = ['Image', 'Mask', 'Predicted Mask']\n",
    "image_list = [X[data_len // 2:][j], y[data_len // 2:][j], np.squeeze(pred[j].argmax(axis=-1))]\n",
    "for i in range(3):\n",
    "    ax[i].imshow(image_list[i], cmap=\"gray\")\n",
    "    ax[i].set_title(subtitles[i])              \n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.multiply(image_list[0], np.expand_dims(image_list[-1], axis=-1)), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1108926,
     "sourceId": 5506221,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
